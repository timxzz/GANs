{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward connection after conv2 before deconv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/ftp/arxiv/papers/1708/1708.09126.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.misc as scm\n",
    "import numpy as np\n",
    "\n",
    "def make_project_dir(project_dir):\n",
    "    if not os.path.exists(project_dir):\n",
    "        os.makedirs(project_dir)\n",
    "        os.makedirs(os.path.join(project_dir, 'models'))\n",
    "        os.makedirs(os.path.join(project_dir, 'result'))\n",
    "        os.makedirs(os.path.join(project_dir, 'result_test'))\n",
    "\n",
    "\n",
    "def get_image(img_path): # [0,255] to [-1,1]\n",
    "    img = scm.imread(img_path) * 2. /255. - 1.\n",
    "    img = img[..., ::-1]  # rgb to bgr\n",
    "    return img\n",
    "\n",
    "def get_label(path, size):\n",
    "    label = int(path[-5])\n",
    "    one_hot = np.zeros(size)\n",
    "    one_hot[ label ] = 0.9\n",
    "    one_hot[ one_hot==0 ] = -0.9\n",
    "    return one_hot\n",
    "\n",
    "def inverse_image(img): # [-1,1] to [0,255]\n",
    "    img = (img + 1.) / 2. * 255.\n",
    "    img[img > 255] = 255\n",
    "    img[img < 0] = 0\n",
    "    img = img[..., ::-1] # bgr to rgb\n",
    "    return img\n",
    "\n",
    "def pair_expressions(paths):\n",
    "    subject_exprs = []\n",
    "    subject_pairs = []\n",
    "    all_pairs = []\n",
    "    last_subject = 0\n",
    "\n",
    "    # Pair all expression of a subject\n",
    "    for path in paths:\n",
    "        subject = int(path[-9:-6])\n",
    "\n",
    "        if subject != last_subject and last_subject != 0:\n",
    "            subject_pairs = [(x, y) for x in subject_exprs for y in subject_exprs]\n",
    "            all_pairs.extend(subject_pairs)\n",
    "            subject_exprs = []\n",
    "\n",
    "        subject_exprs.append(path)\n",
    "        last_subject = subject\n",
    "\n",
    "    # Last subject\n",
    "    subject_pairs = [(x, y) for x in subject_exprs for y in subject_exprs]\n",
    "    all_pairs.extend(subject_pairs)\n",
    "    return all_pairs\n",
    "\n",
    "def get_shape_c(tensor): # static shape\n",
    "    return tensor.get_shape().as_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits\n",
    "\n",
    "def conv(x, filter_shape, bias=True, stride=1, padding=\"VALID\", name=\"conv2d\"):\n",
    "    kw, kh, nin, nout = filter_shape\n",
    "\n",
    "    stddev = np.sqrt(2.0/(np.sqrt(nin*nout)*kw*kh))\n",
    "    k_initializer = tf.truncated_normal_initializer(stddev=stddev)\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.layers.conv2d(x, filters=nout, kernel_size=(kw, kh), strides=(stride, stride), padding=padding, \n",
    "                             use_bias=bias, kernel_initializer=k_initializer)\n",
    "    return x\n",
    "\n",
    "def deconv(x, filter_shape, bias=True, stride=1, padding=\"VALID\", name=\"conv2d_transpose\"):\n",
    "    kw, kh, nin, nout = filter_shape\n",
    "\n",
    "    stddev = np.sqrt(1.0/(np.sqrt(nin*nout)*kw*kh))\n",
    "    k_initializer = tf.random_normal_initializer(stddev=stddev)\n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.layers.conv2d_transpose(x, filters=nout, kernel_size=(kw, kh), strides=(stride, stride), padding=padding, \n",
    "                                       use_bias=bias, kernel_initializer=k_initializer)\n",
    "    return x\n",
    "\n",
    "def fc(x, output_shape, bias=True, name='fc'):\n",
    "    shape = x.get_shape().as_list()\n",
    "    dim = np.prod(shape[1:])\n",
    "    x = tf.reshape(x, [-1, dim])\n",
    "    input_shape = dim\n",
    "\n",
    "    stddev = np.sqrt(1.0/(np.sqrt(input_shape*output_shape)))\n",
    "    initializer = tf.random_normal_initializer(stddev=stddev)\n",
    "    with tf.variable_scope(name):\n",
    "        weight = tf.get_variable(\"weight\", shape=[input_shape, output_shape], initializer=initializer)\n",
    "        x = tf.matmul(x, weight)\n",
    "\n",
    "        if bias:\n",
    "            b = tf.get_variable(\"bias\", shape=[output_shape], initializer=tf.constant_initializer(0.))\n",
    "            x = tf.nn.bias_add(x, b)\n",
    "    return x\n",
    "\n",
    "\n",
    "def pool(x, r=2, s=1):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, r, r, 1], strides=[1, s, s, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "def l1_loss(x, y):\n",
    "    return tf.reduce_mean(tf.abs(x - y))\n",
    "\n",
    "def l2_loss(x, y):\n",
    "    return tf.reduce_mean(tf.square(x - y))\n",
    "\n",
    "def resize_nn(x, size):\n",
    "    return tf.image.resize_nearest_neighbor(x, size=(int(size), int(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class op_base:\n",
    "    def __init__(self, sess, project_name):\n",
    "        self.sess = sess\n",
    "\n",
    "        # Train\n",
    "        self.flag = True #args.flag\n",
    "        self.gpu_number = 0 #args.gpu_number\n",
    "        self.project = project_name #\"test_began\" #args.project\n",
    "\n",
    "        # Train Data\n",
    "        self.data_dir = \"./Face_data/Faces_with_expression_label/dataset_64x64\" #args.data_dir #./Data\n",
    "        self.dataset = \"expr\" #args.dataset  # celeba\n",
    "        self.data_size = 64 #args.data_size  # 64 or 128\n",
    "        self.data_opt = \"crop\" #args.data_opt  # raw or crop\n",
    "        self.data_label_vector_size = 7 #size of one-hot-encoded label vector\n",
    "\n",
    "        # Train Iteration\n",
    "        self.niter = 5000 #50 #args.niter\n",
    "        self.niter_snapshot = 500 #args.nsnapshot\n",
    "        self.max_to_keep = 5 #args.max_to_keep\n",
    "\n",
    "        # Train Parameter\n",
    "        self.batch_size = 32 #16 #args.batch_size\n",
    "        self.learning_rate = 1e-4 #args.learning_rate\n",
    "        self.mm = 0.5 #args.momentum\n",
    "        self.mm2 = 0.999 #args.momentum2\n",
    "        self.lamda = 0.001 #args.lamda\n",
    "        self.gamma = 0.5 #args.gamma\n",
    "        self.filter_number = 64 #args.filter_number\n",
    "        self.input_size = 64 #args.input_size\n",
    "        self.embedding = 128 #64 #args.embedding\n",
    "        self.alpha_1 = 100.0 # AE_L2_loss\n",
    "        self.alpha_2 = 1.0 # AE_D_z_loss\n",
    "        self.alpha_3 = 1.0 # AE_D_img_loss\n",
    "        \n",
    "\n",
    "        # Result Dir & File\n",
    "        self.project_dir = 'assets_ae/{0}_{1}_{2}_{3}/'.format(self.project, self.dataset, self.data_opt, self.data_size)\n",
    "        self.ckpt_dir = os.path.join(self.project_dir, 'models')\n",
    "        self.model_name = \"{0}.model\".format(self.project)\n",
    "        self.ckpt_model_name = os.path.join(self.ckpt_dir, self.model_name)\n",
    "\n",
    "        # etc.\n",
    "        if not os.path.exists('assets_ae'):\n",
    "            os.makedirs('assets_ae')\n",
    "        make_project_dir(self.project_dir)\n",
    "\n",
    "    def load(self, sess, saver, ckpt_dir):\n",
    "        ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, os.path.join(ckpt_dir, ckpt_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "class Operator(op_base):\n",
    "    def __init__(self, sess, project_name):\n",
    "        op_base.__init__(self, sess, project_name)\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Input placeholder\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, self.data_size, self.data_size, 3], name='x')\n",
    "        self.y = tf.placeholder(tf.float32, shape=[None, self.data_size, self.data_size, 3], name='y')\n",
    "        self.c = tf.placeholder(tf.float32, shape=[None, self.data_label_vector_size], name='c')\n",
    "        self.pz = tf.placeholder(tf.float32, shape=[None, self.embedding], name='z_prior')\n",
    "        self.lr = tf.placeholder(tf.float32, name='lr')\n",
    "\n",
    "        # AE\n",
    "        self.e1, self.z = self.encoder(self.x)\n",
    "        self.ae = self.decoder(self.e1, self.z, self.c)\n",
    "        \n",
    "        self.e1_test, self.z_test = self.encoder(self.x, reuse=True)\n",
    "        self.ae_test = self.decoder(self.e1_test, self.z_test, self.c, reuse=True)\n",
    "        \n",
    "        # Discriminators\n",
    "        self.D_z, self.D_z_logits = self.discriminator_z(self.z)\n",
    "        self.D_pz, self.D_pz_logits = self.discriminator_z(self.pz, reuse=True)\n",
    "        \n",
    "        self.D_ae, self.D_ae_logits = self.discriminator_img(self.ae, self.c)\n",
    "        self.D_real, self.D_real_logits = self.discriminator_img(self.y, self.c, reuse=True)\n",
    "\n",
    "        # Loss in AE\n",
    "        self.ae_l2_loss = l2_loss(self.y, self.ae)\n",
    "        # Loss in D_z\n",
    "        self.d_z_loss = self.alpha_2 * (\n",
    "                        tf.reduce_mean(cross_entropy(logits=self.D_pz_logits, labels=tf.ones_like(self.D_pz_logits))) \n",
    "                        + tf.reduce_mean(cross_entropy(logits=self.D_z_logits, labels=tf.zeros_like(self.D_z_logits))))\n",
    "        self.ae_d_z_loss = tf.reduce_mean(cross_entropy(logits=self.D_z_logits, labels=tf.ones_like(self.D_z_logits)))\n",
    "        # Loss in D_img\n",
    "        self.d_img_loss = self.alpha_3 * (\n",
    "                          tf.reduce_mean(cross_entropy(logits=self.D_real_logits, labels=tf.ones_like(self.D_real_logits)))\n",
    "                          + tf.reduce_mean(cross_entropy(logits=self.D_ae_logits, labels=tf.zeros_like(self.D_ae_logits))))\n",
    "        self.ae_d_img_loss = tf.reduce_mean(cross_entropy(logits=self.D_ae_logits, labels=tf.ones_like(self.D_ae_logits)))\n",
    "        # Total loss for AE\n",
    "        self.ae_loss = self.alpha_1 * self.ae_l2_loss + self.alpha_2 * self.ae_d_z_loss + self.alpha_3 * self.ae_d_img_loss\n",
    "\n",
    "        # Variables\n",
    "        ae_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"ae_\")\n",
    "        d_z_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"discriminator_z\")\n",
    "        d_img_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"discriminator_img\")\n",
    "\n",
    "        # Optimizer\n",
    "        self.opt_ae = tf.train.AdamOptimizer(self.lr, self.mm).minimize(self.ae_loss, var_list=ae_vars)\n",
    "        self.opt_d_z = tf.train.AdamOptimizer(self.lr, self.mm).minimize(self.d_z_loss, var_list=d_z_vars)\n",
    "        self.opt_d_img = tf.train.AdamOptimizer(self.lr, self.mm).minimize(self.d_img_loss, var_list=d_img_vars)\n",
    "\n",
    "        # initializer\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # tf saver\n",
    "        self.saver = tf.train.Saver(max_to_keep=(self.max_to_keep))\n",
    "\n",
    "        try:\n",
    "            self.load(self.sess, self.saver, self.ckpt_dir)\n",
    "        except:\n",
    "            # save full graph\n",
    "            self.saver.save(self.sess, self.ckpt_model_name, write_meta_graph=True)\n",
    "\n",
    "        # Summary\n",
    "        if self.flag:\n",
    "            tf.summary.scalar('loss/ae_loss', self.ae_loss)\n",
    "            tf.summary.scalar('loss/ae_l2_loss', self.ae_l2_loss)\n",
    "            tf.summary.scalar('loss/ae_d_z_loss', self.ae_d_z_loss)\n",
    "            tf.summary.scalar('loss/ae_d_img_loss', self.ae_d_img_loss)\n",
    "            tf.summary.scalar('loss/d_z_loss', self.d_z_loss)\n",
    "            tf.summary.scalar('loss/d_img_loss', self.d_img_loss)\n",
    "            self.merged = tf.summary.merge_all()\n",
    "            self.writer = tf.summary.FileWriter(self.project_dir, self.sess.graph)\n",
    "\n",
    "    def train(self, train_flag):\n",
    "        # load data\n",
    "        data_path = self.data_dir\n",
    "        #test_data_path = \"./Face_data/Faces_with_expression_label/dataset_64x64\" # expressions\n",
    "\n",
    "        if os.path.exists(data_path + '.npy'):\n",
    "            data = np.load(data_path + '.npy')\n",
    "        else:\n",
    "            data = sorted(glob.glob(os.path.join(data_path, \"*.*\")))\n",
    "            np.save(data_path + '.npy', data)\n",
    "            \n",
    "        print('Shuffle ....')\n",
    "        random_order = np.random.permutation(len(data))\n",
    "        data = [data[i] for i in random_order[:]]\n",
    "        print('Shuffle Done')\n",
    "\n",
    "        # initial parameter\n",
    "        start_time = time.time()\n",
    "        lr = np.float32(self.learning_rate)\n",
    "        self.count = 0\n",
    "\n",
    "        for epoch in range(self.niter):\n",
    "            batch_idxs = len(data) // self.batch_size\n",
    "            \n",
    "            for idx in range(0, batch_idxs):\n",
    "                self.count += 1\n",
    "\n",
    "                batch_files = data[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "                batch_inputs = [get_image(batch_file[0]) for batch_file in batch_files]\n",
    "                batch_targets = [get_image(batch_file[1]) for batch_file in batch_files]\n",
    "                batch_target_labels = [get_label(batch_file[1], self.data_label_vector_size) for batch_file in batch_files]\n",
    "#                 batch_z_prior = np.random.uniform(low=-1.0, high=1.0, \n",
    "#                                                  size=[self.batch_size, self.embedding]).astype(np.float32)\n",
    "                batch_z_prior = np.random.normal(size=[self.batch_size, self.embedding]).astype(np.float32)\n",
    "                \n",
    "                # opt & feed list \n",
    "                fetches = [self.opt_ae, self.opt_d_z, self.opt_d_img, \n",
    "                           self.ae_loss, self.ae_l2_loss, self.ae_d_z_loss, self.ae_d_img_loss,\n",
    "                           self.d_z_loss, self.d_img_loss, self.merged]\n",
    "                feed_dict = {self.x: batch_inputs, self.y: batch_targets, self.c: batch_target_labels, \n",
    "                             self.pz: batch_z_prior, self.lr: lr}\n",
    "\n",
    "                # run tensorflow\n",
    "                _, _, _, loss_ae, loss_ae_l2, loss_ae_d_z, loss_ae_d_img, loss_d_z, loss_d_img, summary = self.sess.run(\n",
    "                    fetches, feed_dict=feed_dict)\n",
    "                  \n",
    "                if self.count % 100 == 1:\n",
    "                    print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, \"\n",
    "                          \"loss_ae: %.6f, loss_ae_l2: %.6f, loss_ae_d_z: %.6f, loss_ae_d_img: %.6f, \"\n",
    "                          \"loss_d_z: %.6f, loss_d_img: %.6f\"\n",
    "                          % (epoch, idx, batch_idxs, time.time() - start_time,\n",
    "                             loss_ae, loss_ae_l2, loss_ae_d_z, loss_ae_d_img, loss_d_z, loss_d_img))\n",
    "\n",
    "                # write train summary\n",
    "                self.writer.add_summary(summary, self.count)\n",
    "\n",
    "                # Test during Training\n",
    "                if self.count % self.niter_snapshot == (self.niter_snapshot - 1):\n",
    "                    # save & test\n",
    "                    self.saver.save(self.sess, self.ckpt_model_name, global_step=self.count, write_meta_graph=False)\n",
    "                    self.test_expr(train_flag)\n",
    "                    self.test_celebra(train_flag)\n",
    "\n",
    "    def test_celebra(self, train_flag=True):\n",
    "        print('Test Sample Generation...')\n",
    "        # generate output\n",
    "        img_num = 8*8\n",
    "        output_f = int(np.sqrt(img_num))\n",
    "        in_img_num = output_f\n",
    "        img_size = self.data_size\n",
    "        gen_img_num = img_num - output_f\n",
    "        label_size = self.data_label_vector_size\n",
    "        \n",
    "        # load data test\n",
    "        data_path = \"./Face_data/Celeba/dataset_64x64\" # expressions\n",
    "\n",
    "        if os.path.exists(data_path + '.npy'):\n",
    "            data = np.load(data_path + '.npy')\n",
    "        else:\n",
    "            data = sorted(glob.glob(os.path.join(data_path, \"*.*\")))\n",
    "            np.save(data_path + '.npy', data)\n",
    "\n",
    "        # shuffle test data\n",
    "        random_order = np.random.permutation(len(data))\n",
    "        data = [data[i] for i in random_order[:]]\n",
    "\n",
    "        im_output_gen = np.zeros([img_size * output_f, img_size * output_f, 3])\n",
    "\n",
    "        test_files = data[0: output_f]\n",
    "        test_data = [get_image(test_file) for test_file in test_files]\n",
    "        test_data = np.repeat(test_data, [label_size]*in_img_num, axis=0)\n",
    "        test_data_o = [scm.imread(test_file) for test_file in test_files]\n",
    "        \n",
    "        # get one-hot labels\n",
    "        int_labels = list(range(label_size))\n",
    "        one_hot = np.zeros((label_size, label_size))\n",
    "        one_hot[np.arange(label_size), int_labels] = 1\n",
    "        target_labels = np.tile(one_hot, (output_f, 1))\n",
    "        \n",
    "        \n",
    "        output_gen = (self.sess.run(self.ae_test, feed_dict={self.x: test_data, \n",
    "                                                             self.c: target_labels}))  # generator output\n",
    "\n",
    "        output_gen = [inverse_image(output_gen[i]) for i in range(gen_img_num)]\n",
    "\n",
    "        for i in range(output_f):\n",
    "            for j in range(output_f):\n",
    "                if j == 0:\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = test_data_o[i]\n",
    "                else:\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = output_gen[(j-1) + (i * int(output_f-1))]\n",
    "\n",
    "        # output save\n",
    "        scm.imsave(self.project_dir + '/result/' + str(self.count) + '_celebra_output.bmp', im_output_gen)\n",
    "        \n",
    "    def test_expr(self, train_flag=True):\n",
    "        print('Train Sample Generation...')\n",
    "        # generate output\n",
    "        img_num =  36 #self.batch_size\n",
    "        display_img_num = int(img_num / 3)\n",
    "        img_size = self.data_size\n",
    "\n",
    "        output_f = int(np.sqrt(img_num))\n",
    "        im_output_gen = np.zeros([img_size * output_f, img_size * output_f, 3])\n",
    "        \n",
    "        # load data\n",
    "        data_path = self.data_dir\n",
    "\n",
    "        if os.path.exists(data_path + '.npy'):\n",
    "            data = np.load(data_path + '.npy')\n",
    "        else:\n",
    "            data = sorted(glob.glob(os.path.join(data_path, \"*.*\")))\n",
    "            data = pair_expressions(data)\n",
    "            np.save(data_path + '.npy', data)\n",
    "\n",
    "        # Test data shuffle\n",
    "        random_order = np.random.permutation(len(data))\n",
    "        data = [data[i] for i in random_order[:]]\n",
    "        \n",
    "        batch_files = data[0: display_img_num]\n",
    "        test_inputs = [get_image(batch_file[0]) for batch_file in batch_files]\n",
    "        test_inputs_o = [scm.imread((batch_file[0])) for batch_file in batch_files]\n",
    "        test_targets = [scm.imread((batch_file[1])) for batch_file in batch_files]\n",
    "        test_target_labels = [get_label(batch_file[1], self.data_label_vector_size) for batch_file in batch_files]\n",
    "\n",
    "        output_gen = (self.sess.run(self.ae_test, feed_dict={self.x: test_inputs, \n",
    "                                                             self.c: test_target_labels}))  # generator output\n",
    "\n",
    "        output_gen = [inverse_image(output_gen[i]) for i in range(display_img_num)]\n",
    "\n",
    "        for i in range(output_f): # row\n",
    "            for j in range(output_f): # col\n",
    "                if j % 3 == 0: # input img\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = test_inputs_o[int(j / 3) + (i * int(output_f / 3))]\n",
    "                elif j % 3 == 1: # output img\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = output_gen[int(j / 3) + (i * int(output_f / 3))]\n",
    "                else: # target img\n",
    "                    im_output_gen[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :] \\\n",
    "                        = test_targets[int(j / 3) + (i * int(output_f / 3))]\n",
    "                   \n",
    "\n",
    "        labels = np.argmax(test_target_labels, axis=1)\n",
    "        label_string = ''.join(str(int(l)) for l in labels)\n",
    "        # output save\n",
    "        scm.imsave(self.project_dir + '/result/' + str(self.count) + '_' + label_string \n",
    "                   + '_expr_output.bmp', im_output_gen)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AE(Operator):\n",
    "    def __init__(self, sess, project_name):\n",
    "        Operator.__init__(self, sess, project_name)\n",
    "\n",
    "\n",
    "    def encoder(self, x, reuse=None):\n",
    "        with tf.variable_scope('ae_encoder') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            f = self.filter_number\n",
    "            h = self.embedding\n",
    "            p = \"SAME\"\n",
    "\n",
    "            x = conv(x, [5, 5, 3, f], stride=2, padding=p, name='conv1_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv(x, [5, 5, f, f], stride=1, padding=p, name='conv1_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = conv(x, [5, 5, f, f*2], stride=2, padding=p, name='conv2_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv(x, [5, 5, f*2, f*2], stride=1, padding=p, name='conv2_b')\n",
    "            x = tf.nn.elu(x)\n",
    "            \n",
    "            x_1 = x # feed forward\n",
    "            \n",
    "            x = conv(x, [5, 5, f*2, f*4], stride=2, padding=p, name='conv3_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv(x, [5, 5, f*4, f*4], stride=1, padding=p, name='conv3_b')\n",
    "            x = tf.nn.elu(x)\n",
    "            \n",
    "            x = conv(x, [5, 5, f*4, f*4], stride=1, padding=p, name='conv4_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv(x, [5, 5, f*4, f*4], stride=1, padding=p, name='conv4_b')\n",
    "            x = tf.nn.elu(x)\n",
    "\n",
    "            x = conv(x, [5, 5, f*4, f*8], stride=2, padding=p, name='conv5_a')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv(x, [5, 5, f*8, f*8], stride=1, padding=p, name='conv5_b')\n",
    "            x = tf.nn.elu(x)\n",
    "            \n",
    "            x = tf.reshape(x, [-1, 1, 1, 4*4*f*8])\n",
    "            \n",
    "            x = fc(x, h, name='fc')\n",
    "            x = tf.nn.tanh(x)\n",
    "            \n",
    "        return x_1, x\n",
    "\n",
    "    def decoder(self, x_1, x, c, reuse=None):\n",
    "        with tf.variable_scope('ae_decoder') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            w = self.data_size\n",
    "            f = self.filter_number\n",
    "            p = \"SAME\"\n",
    "            \n",
    "            x = tf.concat([x, c], axis=1) # adding label information\n",
    "\n",
    "            x = fc(x, 4*4*f*8, name='fc')\n",
    "            x = tf.reshape(x, [-1, 4, 4, f*8])\n",
    "\n",
    "            x = deconv(x, [5, 5, f*8, f*4], stride=2, padding=p, name='deconv1')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv(x, [5, 5, f*4, f*4], stride=1, padding=p, name='conv1')\n",
    "            x = tf.nn.elu(x)\n",
    "            \n",
    "            x = deconv(x, [5, 5, f*4, f*2], stride=2, padding=p, name='deconv2')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv(x, [5, 5, f*2, f*2], stride=1, padding=p, name='conv2')\n",
    "            x = tf.nn.elu(x)\n",
    "            \n",
    "            x = deconv(x, [5, 5, f*2, f*2], stride=1, padding=p, name='deconv3')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv(x, [5, 5, f*2, f*2], stride=1, padding=p, name='conv3')\n",
    "            x = tf.nn.elu(x)\n",
    "            \n",
    "            x = x + x_1\n",
    "            \n",
    "            x = deconv(x, [5, 5, f*2, f*2], stride=2, padding=p, name='deconv4')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = conv(x, [5, 5, f*2, f], stride=1, padding=p, name='conv4')\n",
    "            x = tf.nn.elu(x)\n",
    "            \n",
    "            x = deconv(x, [5, 5, f, f], stride=2, padding=p, name='deconv5')\n",
    "            x = conv(x, [5, 5, f, 3], stride=1, padding=p, name='conv5')\n",
    "            x = tf.nn.elu(x)\n",
    "            x = tf.nn.tanh(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def discriminator_z(self, z, reuse=None):\n",
    "        with tf.variable_scope('discriminator_z') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "                \n",
    "            x = fc(z, 256, name='fc_1')\n",
    "            x = fc(x, 128, name='fc_2')\n",
    "            x = fc(x, 64, name='fc_3')\n",
    "            x = fc(x, 32, name='fc_4')\n",
    "            x = fc(x, 16, name='fc_5')\n",
    "            x = fc(x, 1, name='fc_6')\n",
    "            \n",
    "        return tf.nn.sigmoid(x), x\n",
    "    \n",
    "    def discriminator_img(self, img, c, reuse=None):\n",
    "        with tf.variable_scope('discriminator_img') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "            \n",
    "            f = 16\n",
    "            c_num = self.data_label_vector_size\n",
    "            p = \"SAME\"\n",
    "\n",
    "            x = conv(img, [5, 5, 3, f], stride=2, padding=p, name='conv1')\n",
    "            x = tf.nn.elu(x)\n",
    "            \n",
    "            # Add classes as channels to the outcome of first layer as described in IcGAN paper\n",
    "            x = tf.concat([x, tf.tile(tf.reshape(c, [-1, 1, 1, get_shape_c(c)[-1]]),\\\n",
    "                                      [1, x.get_shape().as_list()[1], x.get_shape().as_list()[2], 1])],\\\n",
    "                                      axis=3)\n",
    "\n",
    "            x = conv(x, [5, 5, f+c_num, f*2], stride=2, padding=p, name='conv2')\n",
    "            x = tf.nn.elu(x)\n",
    "            \n",
    "            x = conv(x, [5, 5, f*2, f*4], stride=2, padding=p, name='conv3')\n",
    "            x = tf.nn.elu(x)\n",
    "            \n",
    "            x = conv(x, [5, 5, f*4, f*8], stride=2, padding=p, name='conv4')\n",
    "            x = tf.nn.elu(x)\n",
    "            \n",
    "            x = tf.reshape(x, [-1, 1, 1, 4*4*f*8])\n",
    "            \n",
    "            x = fc(x, 1024, name='fc_1')\n",
    "            x = fc(x, 1, name='fc_2')\n",
    "            \n",
    "        return tf.nn.sigmoid(x), x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle ....\n",
      "Shuffle Done\n",
      "Epoch: [ 0] [   0/ 316] time: 1.0566, loss_ae: 38.678905, loss_ae_l2: 0.358783, loss_ae_d_z: 0.548716, loss_ae_d_img: 2.251904, loss_d_z: 1.965354, loss_d_img: 2.244192\n",
      "Epoch: [ 0] [ 100/ 316] time: 14.0615, loss_ae: 10.125440, loss_ae_l2: 0.051564, loss_ae_d_z: 3.880579, loss_ae_d_img: 1.088420, loss_d_z: 0.849079, loss_d_img: 2.082426\n",
      "Epoch: [ 0] [ 200/ 316] time: 27.0960, loss_ae: 10.278779, loss_ae_l2: 0.054225, loss_ae_d_z: 4.398956, loss_ae_d_img: 0.457320, loss_d_z: 0.573631, loss_d_img: 1.944749\n",
      "Epoch: [ 0] [ 300/ 316] time: 40.1665, loss_ae: 10.739998, loss_ae_l2: 0.052203, loss_ae_d_z: 4.759795, loss_ae_d_img: 0.759923, loss_d_z: 0.264183, loss_d_img: 1.401215\n",
      "Epoch: [ 1] [  84/ 316] time: 53.2691, loss_ae: 11.462908, loss_ae_l2: 0.063019, loss_ae_d_z: 4.656425, loss_ae_d_img: 0.504548, loss_d_z: 0.196512, loss_d_img: 1.583944\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 1] [ 184/ 316] time: 67.1193, loss_ae: 11.271939, loss_ae_l2: 0.063087, loss_ae_d_z: 4.667398, loss_ae_d_img: 0.295848, loss_d_z: 0.302206, loss_d_img: 1.811827\n",
      "Epoch: [ 1] [ 284/ 316] time: 80.2697, loss_ae: 10.144800, loss_ae_l2: 0.041776, loss_ae_d_z: 5.160084, loss_ae_d_img: 0.807079, loss_d_z: 0.085224, loss_d_img: 1.461785\n",
      "Epoch: [ 2] [  68/ 316] time: 93.4586, loss_ae: 13.160995, loss_ae_l2: 0.071845, loss_ae_d_z: 5.511753, loss_ae_d_img: 0.464716, loss_d_z: 0.078265, loss_d_img: 1.708621\n",
      "Epoch: [ 2] [ 168/ 316] time: 106.6627, loss_ae: 10.640678, loss_ae_l2: 0.034671, loss_ae_d_z: 5.776180, loss_ae_d_img: 1.397414, loss_d_z: 0.049621, loss_d_img: 1.292737\n",
      "Epoch: [ 2] [ 268/ 316] time: 119.8678, loss_ae: 11.246010, loss_ae_l2: 0.039772, loss_ae_d_z: 6.060179, loss_ae_d_img: 1.208674, loss_d_z: 0.022114, loss_d_img: 1.107980\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 3] [  52/ 316] time: 133.5452, loss_ae: 11.889245, loss_ae_l2: 0.045788, loss_ae_d_z: 6.242129, loss_ae_d_img: 1.068286, loss_d_z: 0.018987, loss_d_img: 0.913898\n",
      "Epoch: [ 3] [ 152/ 316] time: 146.9402, loss_ae: 16.478540, loss_ae_l2: 0.089644, loss_ae_d_z: 6.441863, loss_ae_d_img: 1.072318, loss_d_z: 0.023496, loss_d_img: 1.076486\n",
      "Epoch: [ 3] [ 252/ 316] time: 160.4224, loss_ae: 14.384688, loss_ae_l2: 0.060703, loss_ae_d_z: 6.649796, loss_ae_d_img: 1.664572, loss_d_z: 0.012358, loss_d_img: 1.059100\n",
      "Epoch: [ 4] [  36/ 316] time: 173.8541, loss_ae: 14.381518, loss_ae_l2: 0.059707, loss_ae_d_z: 6.896401, loss_ae_d_img: 1.514377, loss_d_z: 0.009261, loss_d_img: 1.086076\n",
      "Epoch: [ 4] [ 136/ 316] time: 187.4159, loss_ae: 13.144527, loss_ae_l2: 0.045230, loss_ae_d_z: 6.875587, loss_ae_d_img: 1.745975, loss_d_z: 0.011973, loss_d_img: 0.695547\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 4] [ 236/ 316] time: 201.3778, loss_ae: 16.199041, loss_ae_l2: 0.075826, loss_ae_d_z: 7.106779, loss_ae_d_img: 1.509638, loss_d_z: 0.007923, loss_d_img: 0.826496\n",
      "Epoch: [ 5] [  20/ 316] time: 214.9461, loss_ae: 13.995583, loss_ae_l2: 0.049693, loss_ae_d_z: 7.279888, loss_ae_d_img: 1.746351, loss_d_z: 0.008068, loss_d_img: 0.769204\n",
      "Epoch: [ 5] [ 120/ 316] time: 228.5253, loss_ae: 15.956391, loss_ae_l2: 0.066891, loss_ae_d_z: 7.393507, loss_ae_d_img: 1.873796, loss_d_z: 0.006110, loss_d_img: 1.062100\n",
      "Epoch: [ 5] [ 220/ 316] time: 242.1090, loss_ae: 15.751452, loss_ae_l2: 0.057001, loss_ae_d_z: 7.599702, loss_ae_d_img: 2.451693, loss_d_z: 0.003334, loss_d_img: 0.705209\n",
      "Epoch: [ 6] [   4/ 316] time: 255.8640, loss_ae: 14.061521, loss_ae_l2: 0.034992, loss_ae_d_z: 7.749302, loss_ae_d_img: 2.813022, loss_d_z: 0.002718, loss_d_img: 0.893088\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 6] [ 104/ 316] time: 269.8970, loss_ae: 14.176022, loss_ae_l2: 0.046866, loss_ae_d_z: 7.756392, loss_ae_d_img: 1.733012, loss_d_z: 0.005068, loss_d_img: 0.793873\n",
      "Epoch: [ 6] [ 204/ 316] time: 283.6676, loss_ae: 17.591261, loss_ae_l2: 0.080191, loss_ae_d_z: 7.917096, loss_ae_d_img: 1.655031, loss_d_z: 0.003903, loss_d_img: 0.874699\n",
      "Epoch: [ 6] [ 304/ 316] time: 297.4279, loss_ae: 17.300459, loss_ae_l2: 0.068953, loss_ae_d_z: 7.988513, loss_ae_d_img: 2.416651, loss_d_z: 0.002401, loss_d_img: 0.710725\n",
      "Epoch: [ 7] [  88/ 316] time: 311.1449, loss_ae: 14.949619, loss_ae_l2: 0.043372, loss_ae_d_z: 8.158710, loss_ae_d_img: 2.453753, loss_d_z: 0.002766, loss_d_img: 0.739633\n",
      "Epoch: [ 7] [ 188/ 316] time: 324.8442, loss_ae: 20.053061, loss_ae_l2: 0.085781, loss_ae_d_z: 8.340355, loss_ae_d_img: 3.134645, loss_d_z: 0.001902, loss_d_img: 0.850632\n",
      "Train Sample Generation...\n",
      "Test Sample Generation...\n",
      "Epoch: [ 7] [ 288/ 316] time: 338.8800, loss_ae: 175.211823, loss_ae_l2: 1.337421, loss_ae_d_z: 7.012170, loss_ae_d_img: 34.457584, loss_d_z: 0.006638, loss_d_img: 0.021058\n",
      "Epoch: [ 8] [  72/ 316] time: 352.5244, loss_ae: 159.632095, loss_ae_l2: 1.264988, loss_ae_d_z: 7.506495, loss_ae_d_img: 25.626753, loss_d_z: 0.002646, loss_d_img: 0.000000\n",
      "Epoch: [ 8] [ 172/ 316] time: 366.0465, loss_ae: 156.556122, loss_ae_l2: 1.229956, loss_ae_d_z: 7.865669, loss_ae_d_img: 25.694870, loss_d_z: 0.001185, loss_d_img: 0.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4c1e4462d959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# TRAIN / TEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6784fabe3cf2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_flag)\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;31m# run tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 _, _, _, loss_ae, loss_ae_l2, loss_ae_d_z, loss_ae_d_img, loss_d_z, loss_d_img, summary = self.sess.run(\n\u001b[0;32m--> 125\u001b[0;31m                     fetches, feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/VirtualenvEnvironments/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/VirtualenvEnvironments/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/VirtualenvEnvironments/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/VirtualenvEnvironments/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/VirtualenvEnvironments/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import distutils.util\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "''' config settings '''\n",
    "\n",
    "project_name = \"CDAAE_Face_9_\"\n",
    "train_flag = True\n",
    "\n",
    "'''-----------------'''\n",
    "\n",
    "gpu_number = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #args.gpu_number\n",
    "\n",
    "with tf.device('/gpu:{0}'.format(gpu_number)):\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.90)\n",
    "    config = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model = AE(sess, project_name)\n",
    "\n",
    "        # TRAIN / TEST\n",
    "        if train_flag:\n",
    "            model.train(train_flag)\n",
    "        else:\n",
    "            model.test(train_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
